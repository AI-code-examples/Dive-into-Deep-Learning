# C10. 自然语言处理

1.  介绍如何使用向量表示单词，并且在语料库上训练词向量。
2.  求解「定长到定长」问题：利用词向量求近义词和类比词
3.  求解「不定长到定长」问题：利用词向量分析文本情感，进行文本分类
    1.  使用循环神经网络表征时序数据
    2.  使用卷积神经网络表征时序数据
4.  求解「不定长到不定长」问题：编码器-解码器模型
    1.  束搜索
    2.  注意力机制

## 10.1 词嵌入（Word2Vec）

词向量：是用来表示词的向量或者表征，也可以当作词的特征向量。

-   把词映射为实数域向量的技术也叫做词嵌入（Word Embedding）。

### 10.1.1 One-Hot 的问题

词向量的距离度量方式：余弦相似度 $\frac{\mathbf{x}^T\mathbf{y}}{\|\mathbf{x}\|\|\mathbf{y}\|}\in[-1,1]$

任意两个 One-Hot 向量的「余弦相似度」为0，因此无法进行距离度量；而 Word2Vec 向量则可以度量。

Word2Vec的两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words, CBOW）

### 10.1.2 跳字模型

#### 跳字模型的构建

对于文本序列，模型基于某个词来生成它周围的词。假设句子为：the man loves his son，给定中心词「loves」，生成与它距离不超过2个词的背景词的条件概率：
$$
P("the","man","his","son"|"loves")
$$
假设给定中心词的情况下，背景词的生成是相互独立的。
$$
P("the"|"loves")P("man"|"loves")P("his"|"loves")P("son"|"loves")
$$
在跳字模型中，每个词被表示成两个 $d$ 维向量，用来计算条件概率。假设这个词在词典中索引为 $i$ ，当它为中心词时向量表示为 $\mathbf{v}_i\in\mathcal{R}^d$，而为背景词时向量表示为 $\mathbf{u}_i\in\mathcal{R}^d$。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_o$ 在词典中索引为 $o$，给定中心词生成背景词的条件概率可以通过对向量内积做 Softmax 运算而得到：
$$
P(W_o|W_c)=\frac{\exp(\mathbf{u}_o^T\mathbf{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\mathbf{v}_c)}
$$
其中，词典索引集 $\mathcal{V}=\{0,1,\cdots,|\mathcal{V}|-1\}$。假设给定一个长度为 $T$ 的文本序列，设时间步 $t$ 的词为 $w^{(t)}$。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为 $m$ 时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率：
$$
\prod_{t=1}^T\prod_{-m\leq j\leq m,j\neq0} P(w^{(t+j)}|w^{(t)})
$$

#### 跳字模型的训练

最大似然估计，等价于最小化以下的损失函数：
$$
-\sum_{t=1}^T\sum_{-m\leq j\leq m,j\neq0}\log P(w^{(t+j)}|w^{(t)})
$$
基于随机梯度下降，每次迭代随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。
$$
\begin{align}
\log P(w_o|w_c)&=\mathbf{u}_o^T\mathbf{v}_c-\log(\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\mathbf{v}_c))\\
\frac{\partial\log P(w_o|w_c)}{\partial\mathbf{v}_c}
	&=\mathbf{u}_0-\frac{\sum_{j\in\mathcal{V}}\exp(\mathbf{u}_j^T\mathbf{v}_c)\mathbf{u}_j}{\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\mathbf{v}_c)}\\
	&=\mathbf{u}_0-\sum_{j\in\mathcal{V}}(\frac{\exp(\mathbf{u}_j^T\mathbf{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\mathbf{v}_c)})\mathbf{u}_j\\
	&=\mathbf{u}_o-\sum_{j\in\mathcal{V}}P(w_j|w_c)\mathbf{u}_j
\end{align}
$$
训练结束后，对于词典中的任一索引为 $i$ 的词，均得到该词作为中心词和背景词的两组词向量 $\mathbf{v}_i$ 和 $\mathbf{u}_i$。

在自然语言处理应用中，一般使用跳字模型的中心词向量作为词的表征向量。

### 10.1.3 连续词袋模型

#### 连续词袋模型的构建

基于某中心词在文本序列前向的背景词来生成该中心词。
$$
P("loves"|"the","man","his","son")
$$
连续词袋模型的背景词有多个，因此将这些背景词向量取平均，然后使用和跳字模型一样的方法来计算条件概率。设 $\mathbf{v}_i\in\mathcal{R}^d$ 和 $\mathbf{u}_i\in\mathcal{R}^d$ 分别表示词典中索引为 $i$ 的词作为背景词和中心词的向量。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_{o_1},\cdots,w_{o_{2m}}$ 在词典中的索引为 $o_1,\cdots,o_{2m}$，则给定背景词生成中心词的条件概率为：
$$
P(w_c|w_{o_1},\cdots,w_{o_{2m}})=\frac{\exp(\frac1{2m}\mathbf{u_c^T(\mathbf{v}_{o_1}+\cdots+\mathbf{v}_{o_{2m}})})}{\sum_{i\in\mathcal{V}}\exp(\frac1{2m}\mathbf{u}_i^T(\mathbf{v}_{o_1}+\cdots+\mathbf{v}_{o_{2m}}))}
$$
简化符号，记：$\mathcal{W}_o=\{w_{o_1},\cdots,w_{o_{2m}}\},\bar{\mathbf{v}}_o=(\mathbf{v}_{o_1}+\cdots+\mathbf{v}_{o_{2m}})/(2m)$，得：
$$
P(w_c|\mathcal{W}_o)=\frac{\exp(\mathbf{u}_c^T\bar{\mathbf{v}}_o)}{\sum_{i\in\mathcal{V}\exp(\mathbf{u}_i^T\bar{\mathbf{v}}_o)}}
$$
 给定一个长度为 $T$ 的文本序列，设时间步 $t$ 的词为 $w^{(t)}$，背景窗口大小为 $m$。连续词模型的似然函数是由背景词生成任一中心词的概率：
$$
\prod_{t=1}^T P(w^{(t)}|w^{(t-m)},\cdots,w^{(t-1)},w^{(t+1)},\cdots,w^{(t+m)})
$$

#### 连续词袋模型的训练

最大似然估计等价于最小化损失函数：
$$
-\sum_{t}^T\log P(w^{(t)}|w^{(t-m)},\cdots,w^{(t-1)},w^{(t+1)},\cdots,w^{(t+m)})
$$
计算出条件概率的对数有关任一背景词向量 $\mathbf{v}_{o_i}(i=1,\cdots,2m)$ 的梯度：
$$
\begin{align}
\log P(w_c|\mathcal{W}_o)&=\mathbf{u}_c^T\bar{\mathbf{v}}_o-\log(\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\bar{\mathbf{v}}_o))\\
\frac{\partial\log P(w_c|\mathcal{W}_o)}{\partial\mathbf{v}_{o_i}}
	&=\frac1{2m}(\mathbf{u}_c-\sum_{j\in\mathcal{V}}\frac{\exp(\mathbf{u}_j^T\bar{\mathbf{v}}_o)\mathbf{u}_j}{\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\bar{\mathbf{v}}_o)})\\
	&=\frac1{2m}(\mathbf{u}_c-\sum_{j\in\mathcal{V}}P(w_j|\mathcal{W}_o)\mathbf{u}_j)
\end{align}
$$
与跳字模型的区别：使用连续词袋模型的背景词向量作为词的表征向量

## 10.2 近似训练

以上两种条件概率使用了 Softmax 运算，每一步的梯度计算都包含词典大小数目的项的累加，如果词典较大时就会导致每次的梯度计算开销过大问题。为了降低计算复杂度，使用近似训练方法：负采样（negative sampling）或层序 Softmax（hierarchical Softmax）。

### 10.2.1 跳字模型的负采样

修改原来的目标函数：给定中心词 $w_c$ 的一个背景窗口，把背景词 $w_o$ 出现在这个背景窗口作为一个事件，则计算这个事件的概率：
$$
P(D=1|w_c,w_o)=\sigma(\mathbf{u}_o^T\mathbf{v}_c)\\
\sigma(x)=\frac1{1+\exp(-x)}
$$
最大化文本序列中所有这个事件的联合概率来训练词向量：给定一个长度为 $T$ 的文本序列，设时间步 $t$ 的词为 $w^{(t)}$ 并且背景窗口大小为 $m$，得最大化联合概率为：
$$
\prod_{t=1}^T\prod_{-m\leq j\leq m,j\neq0} P(D=1|w^{(t)},w^{(t+j)})
$$
以上模型中包含的事件只考虑了正类样本，即所有词向量相等并且值为无穷大时，以上的联合概率才被最大化为 1，这样的词向量无意义。因此，可以通过负采样（即添加负类样本）使目标函数更有意义。设背景词 $w_o$ 出现在中心词 $w_c$ 的一个背景窗口为事件 $P$，根据分布 $P(w)$ 采样 $K$ 个未出现在这个背景窗口中的词，即噪声词。设噪声词 $w_k(k=1,\cdots,K)$ 不出现在中心词的背景窗口为事件 $N_k$，假设同时含有正类样本和负类样本的事件 $P,N_1,\cdots,N_K$ 相互独立，则负采样将仅考虑正类样本的联合概率改写为：
$$
\prod_{t=1}^T\prod_{-m\leq j\leq m,j\neq0} P(w^{(t+j)}|w^{(t)})=\prod_{t=1}^T\prod_{-m\leq j\leq m,j\neq0} P(D=1|w^{(t)},w^{(t+j)})\prod_{k=1,w_k\sim P(w)}P(D=0|w^{(t)},w^{(t+j)})
$$

设文本序列中时间步 $t$ 的词 $w^{(t)}$ 在词典中的索引为 $i_t$，噪声词 $w_k$ 在扁中的索引为 $h_k$。则以上条件概率的对数损失为：
$$
\begin{align}
-\log P(w^{(t+j)}|w^{(t)})
	&=-\log P(D=1|w^{(t)},w^{(t+j)})&&-\sum_{k=1,w_k\sim P(w)}^K\log P(D=0|w^{(t)},w_k)\\
	&=-\log\sigma(\mathbf{u}_{i_{t+j}}^T\mathbf{v}_{i_{t}})&&-\sum_{k=1,w_k\sim P(w)}^K\log(1-\sigma(\mathbf{u}_{h_k}^T\mathbf{v}_{i_t}))\\
	&=-\log\sigma(\mathbf{u}_{i_{t+j}}^T)&&-\sum_{k=1,w_k\sim P(w)}^K\log\sigma(-\mathbf{u}_{h_k}^T\mathbf{v}_{i_t})
\end{align}
$$
于是，训练中每一步的梯度计算开销不再与词典大小相关，而与 $K$ 线性相关。当 $K$ 取较小的常数时，负采样在每一步的梯度计算开销较小。

### 10.2.2 层序 Softmax

使用二叉树存储词典，树的每个叶结点代表词典 $\mathcal{V}$ 中的每个词。假设 $L(w)$ 为从二叉树的根结点到词 $w$ 的叶结点的路径上的结点数，路径包括了根结点和叶结点。设 $n(w,j)$ 为这个路径上的第 $j$ 个亍，并且设这个结点的背景词向量为 $\mathbf{u}_{n(w,j)}$，则层序 Softmax 将跳字模型中的条件概率近似表示为：
$$
P(w_o|w_c)=\prod_{j=1}^{L(w_o)-1}\sigma([n(w_o,j+1)=\text{leftChilde}(n(w_o,j))]\cdot\mathbf{u}_{n(w_o,j)}^T\mathbf{v}_c)
$$
$\text{leftChild}(n)$ 是结点 $n$ 的左子结点：如果判断 $x$ 为真，则 $[x]=1$；反之 $[x]=-1$。