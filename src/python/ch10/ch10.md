# C10. 自然语言处理

1.  介绍如何使用向量表示单词，并且在语料库上训练词向量。
2.  求解「定长到定长」问题：利用词向量求近义词和类比词
3.  求解「不定长到定长」问题：利用词向量分析文本情感，进行文本分类
    1.  使用循环神经网络表征时序数据
    2.  使用卷积神经网络表征时序数据
4.  求解「不定长到不定长」问题：编码器-解码器模型
    1.  束搜索
    2.  注意力机制

## 10.1 词嵌入（Word2Vec）

词向量：是用来表示词的向量或者表征，也可以当作词的特征向量。

-   把词映射为实数域向量的技术也叫做词嵌入（Word Embedding）。

### 10.1.1 One-Hot 的问题

词向量的距离度量方式：余弦相似度 $\frac{\mathbf{x}^T\mathbf{y}}{\|\mathbf{x}\|\|\mathbf{y}\|}\in[-1,1]$

任意两个 One-Hot 向量的「余弦相似度」为0，因此无法进行距离度量；而 Word2Vec 向量则可以度量。

Word2Vec的两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words, CBOW）

### 10.1.2 跳字模型

#### 跳字模型的构建

对于文本序列，模型基于某个词来生成它周围的词。假设句子为：the man loves his son，给定中心词「loves」，生成与它距离不超过2个词的背景词的条件概率：
$$
P("the","man","his","son"|"loves")
$$
假设给定中心词的情况下，背景词的生成是相互独立的。
$$
P("the"|"loves")P("man"|"loves")P("his"|"loves")P("son"|"loves")
$$
在跳字模型中，每个词被表示成两个 $d$ 维向量，用来计算条件概率。假设这个词在词典中索引为 $i$ ，当它为中心词时向量表示为 $\mathbf{v}_i\in\mathcal{R}^d$，而为背景词时向量表示为 $\mathbf{u}_i\in\mathcal{R}^d$。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_o$ 在词典中索引为 $o$，给定中心词生成背景词的条件概率可以通过对向量内积做 Softmax 运算而得到：
$$
P(w_o|w_c)=\frac{\exp(\mathbf{u}_o^T\mathbf{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\mathbf{v}_c)}
$$
其中，词典索引集 $\mathcal{V}=\{0,1,\cdots,|\mathcal{V}|-1\}$。假设给定一个长度为 $T$ 的文本序列，设时间步 $t$ 的词为 $w^{(t)}$。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为 $m$ 时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率：
$$
\prod_{t=1}^T\prod_{-m\leq j\leq m,j\neq0} P(w^{(t+j)}|w^{(t)})
$$

#### 跳字模型的训练

最大似然估计，等价于最小化以下的损失函数：
$$
-\sum_{t=1}^T\sum_{-m\leq j\leq m,j\neq0}\log P(w^{(t+j)}|w^{(t)})
$$
基于随机梯度下降，每次迭代随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。
$$
\begin{aligned}
\log P(w_o|w_c)&=\mathbf{u}_o^T\mathbf{v}_c-\log(\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\mathbf{v}_c))\\
\frac{\partial\log P(w_o|w_c)}{\partial\mathbf{v}_c}
	&=\mathbf{u}_0-\frac{\sum_{j\in\mathcal{V}}\exp(\mathbf{u}_j^T\mathbf{v}_c)\mathbf{u}_j}{\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\mathbf{v}_c)}\\
	&=\mathbf{u}_0-\sum_{j\in\mathcal{V}}(\frac{\exp(\mathbf{u}_j^T\mathbf{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\mathbf{v}_c)})\mathbf{u}_j\\
	&=\mathbf{u}_o-\sum_{j\in\mathcal{V}}P(w_j|w_c)\mathbf{u}_j
\end{aligned}
$$
训练结束后，对于词典中的任一索引为 $i$ 的词，均得到该词作为中心词和背景词的两组词向量 $\mathbf{v}_i$ 和 $\mathbf{u}_i$。

在自然语言处理应用中，一般使用跳字模型的中心词向量作为词的表征向量。

### 10.1.3 连续词袋模型

#### 连续词袋模型的构建

基于某中心词在文本序列前向的背景词来生成该中心词。
$$
P("loves"|"the","man","his","son")
$$
连续词袋模型的背景词有多个，因此将这些背景词向量取平均，然后使用和跳字模型一样的方法来计算条件概率。设 $\mathbf{v}_i\in\mathcal{R}^d$ 和 $\mathbf{u}_i\in\mathcal{R}^d$ 分别表示词典中索引为 $i$ 的词作为背景词和中心词的向量。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_{o_1},\cdots,w_{o_{2m}}$ 在词典中的索引为 $o_1,\cdots,o_{2m}$，则给定背景词生成中心词的条件概率为：
$$
P(w_c|w_{o_1},\cdots,w_{o_{2m}})=\frac{\exp(\frac1{2m}\mathbf{u_c^T(\mathbf{v}_{o_1}+\cdots+\mathbf{v}_{o_{2m}})})}{\sum_{i\in\mathcal{V}}\exp(\frac1{2m}\mathbf{u}_i^T(\mathbf{v}_{o_1}+\cdots+\mathbf{v}_{o_{2m}}))}
$$
简化符号，记：$\mathcal{W}_o=\{w_{o_1},\cdots,w_{o_{2m}}\},\bar{\mathbf{v}}_o=(\mathbf{v}_{o_1}+\cdots+\mathbf{v}_{o_{2m}})/(2m)$，得：
$$
P(w_c|\mathcal{W}_o)=\frac{\exp(\mathbf{u}_c^T\bar{\mathbf{v}}_o)}{\sum_{i\in\mathcal{V}\exp(\mathbf{u}_i^T\bar{\mathbf{v}}_o)}}
$$
 给定一个长度为 $T$ 的文本序列，设时间步 $t$ 的词为 $w^{(t)}$，背景窗口大小为 $m$。连续词模型的似然函数是由背景词生成任一中心词的概率：
$$
\prod_{t=1}^T P(w^{(t)}|w^{(t-m)},\cdots,w^{(t-1)},w^{(t+1)},\cdots,w^{(t+m)})
$$

#### 连续词袋模型的训练

最大似然估计等价于最小化损失函数：
$$
-\sum_{t}^T\log P(w^{(t)}|w^{(t-m)},\cdots,w^{(t-1)},w^{(t+1)},\cdots,w^{(t+m)})
$$
计算出条件概率的对数有关任一背景词向量 $\mathbf{v}_{o_i}(i=1,\cdots,2m)$ 的梯度：
$$
\begin{aligned}
\log P(w_c|\mathcal{W}_o)&=\mathbf{u}_c^T\bar{\mathbf{v}}_o-\log(\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\bar{\mathbf{v}}_o))\\
\frac{\partial\log P(w_c|\mathcal{W}_o)}{\partial\mathbf{v}_{o_i}}
	&=\frac1{2m}(\mathbf{u}_c-\sum_{j\in\mathcal{V}}\frac{\exp(\mathbf{u}_j^T\bar{\mathbf{v}}_o)\mathbf{u}_j}{\sum_{i\in\mathcal{V}}\exp(\mathbf{u}_i^T\bar{\mathbf{v}}_o)})\\
	&=\frac1{2m}(\mathbf{u}_c-\sum_{j\in\mathcal{V}}P(w_j|\mathcal{W}_o)\mathbf{u}_j)
\end{aligned}
$$
与跳字模型的区别：使用连续词袋模型的背景词向量作为词的表征向量

### 小结

1.  词向量是用来表示词的向量。把词映射为实数域向量的技术也叫词嵌入
2.  Word2Vec 常用两种模型实现
    -   跳字模型：基于中心词来生成背景词
    -   连续词袋模型：基于背景词来生成中心词

## 10.2 近似训练

以上两种条件概率使用了 Softmax 运算，每一步的梯度计算都包含词典大小数目的项的累加，如果词典较大时就会导致每次的梯度计算开销过大问题。为了降低计算复杂度，使用近似训练方法：负采样（negative sampling）或层序 Softmax（hierarchical Softmax）。

### 10.2.1 跳字模型的负采样

修改原来的目标函数：给定中心词 $w_c$ 的一个背景窗口，把背景词 $w_o$ 出现在这个背景窗口作为一个事件，则计算这个事件的概率：
$$
P(D=1|w_c,w_o)=\sigma(\mathbf{u}_o^T\mathbf{v}_c)\\
\sigma(x)=\frac1{1+\exp(-x)}
$$
最大化文本序列中所有这个事件的联合概率来训练词向量：给定一个长度为 $T$ 的文本序列，设时间步 $t$ 的词为 $w^{(t)}$ 并且背景窗口大小为 $m$，得最大化联合概率为：
$$
\prod_{t=1}^T\prod_{-m\leq j\leq m,j\neq0} P(D=1|w^{(t)},w^{(t+j)})
$$
以上模型中包含的事件只考虑了正类样本，即所有词向量相等并且值为无穷大时，以上的联合概率才被最大化为 1，这样的词向量无意义。因此，可以通过负采样（即添加负类样本）使目标函数更有意义。设背景词 $w_o$ 出现在中心词 $w_c$ 的一个背景窗口为事件 $P$，根据分布 $P(w)$ 采样 $K$ 个未出现在这个背景窗口中的词，即噪声词。设噪声词 $w_k(k=1,\cdots,K)$ 不出现在中心词的背景窗口为事件 $N_k$，假设同时含有正类样本和负类样本的事件 $P,N_1,\cdots,N_K$ 相互独立，则负采样将仅考虑正类样本的联合概率改写为：
$$
\prod_{t=1}^T\prod_{-m\leq j\leq m,j\neq0} P(w^{(t+j)}|w^{(t)})=\prod_{t=1}^T\prod_{-m\leq j\leq m,j\neq0} P(D=1|w^{(t)},w^{(t+j)})\prod_{k=1,w_k\sim P(w)}P(D=0|w^{(t)},w^{(t+j)})
$$

设文本序列中时间步 $t$ 的词 $w^{(t)}$ 在词典中的索引为 $i_t$，噪声词 $w_k$ 在扁中的索引为 $h_k$。则以上条件概率的对数损失为：
$$
\begin{aligned}
-\log P(w^{(t+j)}|w^{(t)})
	&=-\log P(D=1|w^{(t)},w^{(t+j)})&&-\sum_{k=1,w_k\sim P(w)}^K\log P(D=0|w^{(t)},w_k)\\
	&=-\log\sigma(\mathbf{u}_{i_{t+j}}^T\mathbf{v}_{i_{t}})&&-\sum_{k=1,w_k\sim P(w)}^K\log(1-\sigma(\mathbf{u}_{h_k}^T\mathbf{v}_{i_t}))\\
	&=-\log\sigma(\mathbf{u}_{i_{t+j}}^T)&&-\sum_{k=1,w_k\sim P(w)}^K\log\sigma(-\mathbf{u}_{h_k}^T\mathbf{v}_{i_t})
\end{aligned}
$$
于是，训练中每一步的梯度计算开销不再与词典大小相关，而与 $K$ 线性相关。当 $K$ 取较小的常数时，负采样在每一步的梯度计算开销较小。

### 10.2.2 层序 Softmax

使用二叉树存储词典，树的每个叶结点代表词典 $\mathcal{V}$ 中的每个词。假设 $L(w)$ 为从二叉树的根结点到词 $w$ 的叶结点的路径上的结点数，路径包括了根结点和叶结点。设 $n(w,j)$ 为这个路径上的第 $j$ 个亍，并且设这个结点的背景词向量为 $\mathbf{u}_{n(w,j)}$，则层序 Softmax 将跳字模型中的条件概率近似表示为：
$$
P(w_o|w_c)=\prod_{j=1}^{L(w_o)-1}\sigma([n(w_o,j+1)=\text{leftChilde}(n(w_o,j))]\cdot\mathbf{u}_{n(w_o,j)}^T\mathbf{v}_c)
$$
$\text{leftChild}(n)$ 是结点 $n$ 的左子结点：如果判断 $x$ 为真，则 $[x]=1$；反之 $[x]=-1$。

### 小结

1.  负采样通过考虑同时含有正类样本和负类样本的相互独立事件来构造损失函数。其训练中每一步的梯度计算开销与采样的噪声词的个数线性相关
2.  层序 Softmax 使用了二叉树，并且根据根结点到叶结点的路径来构造损失函数，其训练中每一步的梯度计算开销与词典大小的对数相关。


## 10.4 子词嵌入（fastText）

构词学（morphology）：研究的是词的内部结构和形成方式。

在子词嵌入（fastText）中，每个中心词被表示成子词的集合。对于一个词 $w$，将其所有长度在 $3\sim 6$ 的子词和特殊子词的并集记为 $\mathcal{G}_w$，那么词典就是所有词的子词集合的并集。假设词典中子词 $g$ 的向量为 $\mathbf{z}_g$，那么跳字模型中词 $w$ 作为中心词的向量 $\mathbf{v}_w$ 则表示成：$\mathbf{v}_w=\sum_{g\in\mathcal{G}_w}\mathbf{z}_g$。

fastText 的其余部分与跳字模型相同，只是其词典规模更大，造成模型参数更多，同时一个词的向量需要对所有子词向量求和，继而导致计算复杂度更高。同时，fastText对于较生僻的复杂单词，以及词典中没有的单词，可能会从同它结构类似的其他词那里获取更好的词向量表示。

### 小结

1.  fastText 提出了子词嵌入方法。它在 Word2Vec 中的跳字模型的基础上，将中心词向量表示成单词的子词向量之和
2.  子词嵌入利用构词上的规律，可以提升生僻词表征的质量

## 10.5 全局向量的词嵌入（GloVe）

将跳字模型中使用的 Softmax 运算表达的条件概率 $P(w_j|w_i)$ 记作 $q_{ij}$，即
$$
q_{ij}=\frac{\exp(\mathbf{u}_j^T\mathbf{v}_i)}{\sum_{k\in\mathcal{V}}\exp(\mathbf{u}_k^T\mathbf{v}_i)}
$$
索引为 $i$ 的词 $w_i$ 的中心词向量为 $\mathbf{v}_i$、背景词向量为 $\mathbf{u}_i$；词典索引集：$\mathcal{V}=\{0，1，\cdots,|\mathcal{V}|-1\}$。

对于词 $w_i$，将每一次以它为中心词的所有背景词全部汇总，并且保留重复的元素，组成一个集合，记为「多重集（multiset）」$\mathcal{C}_i$。

在多重集 $\mathcal{C}_i$ 中某个元素 $w_j$ 重复的次数记为这个元素的重数（multiplicity）$x_{ij}$。

于是，跳字模型的损失函数的表达方式可以记为：
$$
-\sum_{i\in\mathcal{V}}\sum_{j\in\mathcal{V}} x_{ij}\log q_{ij}
$$
将数据集中所有以词 $w_i$ 为中心词的背景词的数量之和 $|\mathcal{C}_i|$ 记为 $x_i$，将以 $w_i$ 为中心词生成背景词 $w_j$ 的条件概率 $P(w_j|w_i)=x_{ij}/x_i$ 记为 $p_{ij}$，则损失函数记为：
$$
-\sum_{i\in\mathcal{V}}x_i\sum_{j\in\mathcal{V}}p_{ij}\log q_{ij}
$$
其中，$\sum_{j\in\mathcal{V}}p_{ij}\log q_{ij}$ 计算的是以 $w_i$ 为中心词的背景词条件概率分布 $p_{ij}$ 和模型预测的条件概率分布 $q_{ij}$ 的交叉熵，并且损失函数使用所有以词 $w_i$ 为中心词的背景词的数量之和来加权。最小化卡式中的损失函数会令预测的条件概率分布尽可能地接近真实的条件概率分布。

存在的困难：

1.  交叉熵损失函数基于模型预测 $q_{ij}$ 因此需要使用整个词典进行计算，从而导致计算开销过大；
2.  词典中存在大量生僻词，在数据集中出现的次数较少，基于大量生僻词的条件概率分布在交叉熵损失函数中的最终预测并不准确。

### 10.5.1 GloVe 模型

GloVe模型采用平方损失函数，并且基于这个损失函数对跳字模型做了三点改变：

1.  使用非概率分布的变量 $p'_{ij}=x_{ij}$ 和 $q'_{ij}=\exp(\mathbf{u}_j^T\mathbf{v}_i)$，并对它们取对数，得平方损失项为 $(\log p'_{ij}-\log q'_{ij})^2=(\mathbf{u}_j^T\mathbf{v}_i-\log x_{ij})$
2.  为每个词 $w_i$ 增加两个标量模型参数：中心词偏差项 $b_i$ 和 背景词偏差项 $c_i$
3.  将每个损失项的权重替换成函数 $h(x_{ij})$。权重函数 $h(x)$ 是值域在 $[0,1]$ 的单词递增函数

于是，GloVe 模型的目标是最小化损失函数：
$$
\sum_{i\in\mathcal{V}}\sum_{j\in\mathcal{V}} h(x_{ij})(\mathbf{u}_j^T\mathbf{v}_i+b_i+c_j-\log x_{ij})^2
$$

权重函数 $h(x)$ 的选择建议：当 $x<c$ 时（例如：$c=100$），令 $h(x)=(x/c)^\alpha$（例如：$\alpha=0.75$），反之令 $h(x)=1$。

GloVe 模型的命名取「全局向量」（Global Vectors）：因为非零 $x_{ij}$ 是预先基于整个数据集计算得到的，包含了数据集的全局统计信息。

GloVe 模型生成的「最终词向量=中心词向量+背景词向量」。

### 10.5.2 从条件概率比值理解 GloVe 模型

以 $w_i$ 为中心词生成背景词 $w_j$ 的条件概率 $P(w_j|w_i)=x_{ij}/x_i$ 记为 $p_{ij}$，通过表10-1可知条件概率的比值能够比较直观地表达词与词之间的关系，因此构造一个词向量函数使它能够有效拟合条件概率比值：
$$
f(\mathbf{u}_j,\mathbf{u}_k,\mathbf{v}_k)\approx\frac{p_{ij}}{p_{ik}}
$$
注：函数的选择并不唯一，这个函数只是合理选择之一。

因为条件概率比值是一个标量，因此将 $f$ 限制为一个标量函数：$f(\mathbf{u}_j,\mathbf{u}_k,\mathbf{v}_i)=f((\mathbf{u}_j-\mathbf{u}_k)^T\mathbf{v}_i)$，利用函数的性质：$f(x)f(-x)=1$，设 $f(x)=\exp(x)$，重新构造函数为：
$$
f(\mathbf{u}_j,\mathbf{u}_k,\mathbf{v}_i)=\frac{\exp(\mathbf{u}_j^T\mathbf{v}_i)}{\exp(\mathbf{u}_k\mathbf{v}_i)}\approx\frac{p_{ij}}{p_{ik}}
$$
注：后面的数学公式推导建议参考论文。

### 小结

1.   GloVe 模型采用平方损失函数替代交叉熵损失函数，并且通过词向量拟合预先基于整个数据集计算得到的全局统计信息
2.  任意词的中心词向量和背景词向量在 GloVe 模型中是等价的