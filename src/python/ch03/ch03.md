# C03. 深度学习基础

## 3.4 Softmax 回归

### 3.4.1 分类问题

### 3.4.2 Softmax 回归模型

#### Softmax 运算

Softmax 运算解决了直接使用输出层输出存在的问题：将输出值变换成正的值，并且所有值的和为 1 的概率分布，因此 Softmax 运算不改变预测类别输出。

### 3.4.3 单样本分类的矢量计算表达式

Softmax 回归对样本 $i$ 分类的矢量计算表达式为：
$$
\begin{aligned}
\mathbf{o}^{(i)}&=\mathbf{x}^{(i)}\mathbf{W}+\mathbf{b}\\
\hat{\mathbf{y}}^{(i)}&=\text{softmax}(\mathbf{o}^{(i)})
\end{aligned}
$$

### 3.4.4 小批量样本分类的矢量计算表达式

给定一个小批量样本，其批量大小为 $n$，输入个数（特征数）为 $d$，输出个数（类别数）为 $q$。设批量特征为 $\mathbf{X}\in\mathcal{R}^{n\times d}$，Softmax 回归的权重为 $\mathbf{W}\in\mathcal{R}^{n\times d}$，Softmax 回归的偏差为 $\mathbf{b}\in\mathcal{R}^{1\tiimes q}$，Softmax 回归的矢量计算表达式为：
$$
\begin{aligned}
\mathbf{O}&=\mathbf{XW}+\mathbf{b},\mathbf{O}\in\mathcal{R}^{n\times q}\\
\hat{\mathbf{Y}}&=\text{softmax}(\mathbf{O}),\mathbf{Y}\in\mathcal{R}^{n\times q}
\end{aligned}
$$
$\mathbf{O}$ 的第 $i$ 行为样本 $i$ 的输出 $\mathbf{o}^{(i)}$

$\mathbf{Y}$ 的第 $i$ 行为样本 $i$ 的概率分布 $\mathbf{y}^{(i)}$

### 3.4.5 交叉熵损失函数

常用的衡量两个概率分布差异的测量函数为交叉熵：
$$
H(\mathbf{y}^{(i)}_j,\log\hat{\mathbf{y}}^{(i)}_j)=-\sum_{j=1}^q y^{(i)}_j\log\hat{\mathbf{y}}^{(i)}_j
$$
假设训练数据集的样本数为 $n$，则交叉熵损失函数的定义为：
$$
l(\mathbf{\Theta})=\frac1n\sum_{i=1}^n H(\mathbf{y}^{(i)},\hat{\mathbf{y}}^{(i)})
$$
最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。

### 3.4.6 模型预测及评价

### 小结

1.  Softmax 回归适用于分类问题，使用 Softmax 函数输出类别的概率分布
2.  Softmax 回归是一个单层神经网络，输出个数等于分类问题中的类别个数
3.  交叉熵适合衡量两个概率分布的差异

## 3.14 计算传播与计算图

### 3.14.1 正向传播

正向传播（forward-propagation）：对神经网络沿着从输入层到输出层的顺序，依次计算并且存储模型的中间变量（包括输出）。

假设输入是特征 $\mathbf{x}\in\mathcal{R}^d$，隐藏层的权重参数为 $\mathbf{W}^{(1)}\in\mathcal{R}^{h\times d}$ ，则中间变量为 $\mathbf{z}=\mathbf{W}^{(1)}\mathbf{x}$。

中间变量 $\mathbf{z}\in\mathcal{R}^h$ 经过激活函数 $\phi$ 得隐藏层变量 $\mathbf{h}=\phi(\mathbf{z})$。

假设输出层的权重参数为 $\mathbf{W}^{(2)}\in\mathcal{R}^{q\times h}$，则输出层变量为 $\mathbf{o}=\mathbf{W}^{(2)}\mathbf{h}$。

假设损失函数为 $l$，并且样本标签为 $y$，则单个数据样本的损失项为 $L=l(\mathbf{o},y)$。

如果使用 $L_2$ 范数正则化，给定超参数 $\lambda$，则正则化项为 $s=\frac\lambda2(\|\mathbf{W}^{(1)})\|_F^2+\|\mathbf{W}^{(2)})\|_F^2)$。

矩阵的 Frobenius 范数等价于将矩阵变平为向量后计算 $L_2$ 范数。

给定数据样本的目标函数为 $J=L+s$，简称目标函数。

### 3.14.2 正向传播的计算图

图3-6 正向传播的计算图

### 3.14.3 反向传播

反向传播（back-propagation）：是计算神经网络参数梯度的方法。
$$
\frac{\partial\mathbf{Z}}{\partial\mathbf{X}}=prod(\frac{\partial\mathbf{Z}}{\partial\mathbf{Y}},\frac{\partial\mathbf{Y}}{\partial\mathbf{X}})
$$
依据链式法则计算目标函数
$$
\begin{aligned}
\frac{\partial\mathbf{J}}{\partial\mathbf{o}}
	&=prod(\frac{\partial\mathbf{J}}{\partial\mathbf{L}},\frac{\partial\mathbf{L}}{\partial\mathbf{o}})
	=\frac{\partial\mathbf{L}}{\partial\mathbf{o}},
	&\frac{\partial\mathbf{J}}{\partial\mathbf{o}}&\in\mathcal{R}^q\\
\frac{\partial\mathbf{J}}{\partial\mathbf{W}^{(2)}}
	&=prod(\frac{\partial\mathbf{J}}{\partial\mathbf{o}},\frac{\partial\mathbf{o}}{\partial\mathbf{W}^{(2)}})
	+prod(\frac{\partial\mathbf{J}}{\partial\mathbf{s}},\frac{\partial\mathbf{s}}{\partial\mathbf{W}^{(2)}})
	=\frac{\partial\mathbf{J}}{\partial\mathbf{o}}\mathbf{h}^T+\lambda\mathbf{W}^{(2)},
	&\frac{\partial\mathbf{J}}{\partial\mathbf{W}^{(2)}}&\in\mathcal{R}^{q\times h}\\
\frac{\partial\mathbf{J}}{\partial\mathbf{h}}
	&=prod(\frac{\partial\mathbf{J}}{\partial\mathbf{o}},\frac{\partial\mathbf{o}}{\partial\mathbf{h}})
	=\mathbf{W}^{(2)T} \frac{\partial\mathbf{J}}{\partial\mathbf{o}},
	&\frac{\partial\mathbf{J}}{\partial\mathbf{h}}&\in\mathcal{R}^h\\
\frac{\partial\mathbf{J}}{\partial\mathbf{z}}
	&=prod(\frac{\partial\mathbf{J}}{\partial\mathbf{h}},\frac{\partial\mathbf{h}}{\partial\mathbf{z}})
	=\frac{\partial\mathbf{J}}{\partial\mathbf{h}}\odot\phi'(\mathbf{z}),
	&\frac{\partial\mathbf{J}}{\partial\mathbf{z}}&\in\mathcal{R}^h\\
\frac{\partial\mathbf{J}}{\partial\mathbf{W}^{(1)}}
	&=prod(\frac{\partial\mathbf{J}}{\partial\mathbf{z}},\frac{\partial\mathbf{z}}{\partial\mathbf{W}^{(1)}})
	+prod(\frac{\partial\mathbf{J}}{\partial\mathbf{s}},\frac{\partial\mathbf{s}}{\partial\mathbf{W}^{(1)}})
	=\frac{\partial\mathbf{J}}{\partial\mathbf{z}}\mathbf{x}^T+\lambda\mathbf{W}^{(1)},
	&\frac{\partial\mathbf{J}}{\partial\mathbf{W}^{(1)}}&\in\mathcal{R}^{h\times d}
\end{aligned}
$$

### 3.14.4 训练深度学习模型

### 小结

1.  正向传播沿着输入层到输出层的顺序，依次计算并且存储神经网络的中间变量
2.  反向传播沿着输出层到输入层的顺序，依次计算并且存储神经网络的中间变量和参数的梯度
3.  在训练深度学习模型时，正向传播和反向传播相互依赖

## 3.15  数值稳定性和模型初始化

### 3.15.1 数值稳定的典型问题：衰减和爆炸

当神经网络的层数较多时，模型的数值稳定性容易变差：模型的参数与梯度计算都会遇到衰减（vanishing）和爆炸（explosion）。

### 3.15.2 随机初始化模型参数

#### MXNet的默认随机初始化

#### Xavier随机初始化

### 小结

1.  深度学习模型的数值稳定性主要面临两个问题：「衰减」和「爆炸」。当神经网络的层数过多时，模型的数值稳定性容易变差
2.  通常随机初始化模型的参数
    1.  MXNet 默认采用随机初始化：权重参数随机采样于(-0.07,0.07)的均匀分布，偏差参数全部清零
    2.  Xavier 随机初始化：权重参数随机采样于 $(-\sqrt{ 6/(a+b) },\sqrt{ 6/(a+b) })$ 的均匀分布
        -   设计思想：模型参数初始化后，每层输出的方差不受该输入个数的影响，每层梯度的方差不受该层输入个数的影响