# C03. 深度学习基础

## 3.4 Softmax 回归

### 3.4.1 分类问题

### 3.4.2 Softmax 回归模型

#### Softmax 运算

Softmax 运算解决了直接使用输出层输出存在的问题：将输出值变换成正的值，并且所有值的和为 1 的概率分布，因此 Softmax 运算不改变预测类别输出。

### 3.4.3 单样本分类的矢量计算表达式

Softmax 回归对样本 $i$ 分类的矢量计算表达式为：
$$
\begin{aligned}
\mathbf{o}^{(i)}&=\mathbf{x}^{(i)}\mathbf{W}+\mathbf{b}\\
\hat{\mathbf{y}}^{(i)}&=\text{softmax}(\mathbf{o}^{(i)})
\end{aligned}
$$

### 3.4.4 小批量样本分类的矢量计算表达式

给定一个小批量样本，其批量大小为 $n$，输入个数（特征数）为 $d$，输出个数（类别数）为 $q$。设批量特征为 $\mathbf{X}\in\mathcal{R}^{n\times d}$，Softmax 回归的权重为 $\mathbf{W}\in\mathcal{R}^{n\times d}$，Softmax 回归的偏差为 $\mathbf{b}\in\mathcal{R}^{1\tiimes q}$，Softmax 回归的矢量计算表达式为：
$$
\begin{aligned}
\mathbf{O}&=\mathbf{XW}+\mathbf{b},\mathbf{O}\in\mathcal{R}^{n\times q}\\
\hat{\mathbf{Y}}&=\text{softmax}(\mathbf{O}),\mathbf{Y}\in\mathcal{R}^{n\times q}
\end{aligned}
$$
$\mathbf{O}$ 的第 $i$ 行为样本 $i$ 的输出 $\mathbf{o}^{(i)}$

$\mathbf{Y}$ 的第 $i$ 行为样本 $i$ 的概率分布 $\mathbf{y}^{(i)}$

### 3.4.5 交叉熵损失函数

常用的衡量两个概率分布差异的测量函数为交叉熵：
$$
H(\mathbf{y}^{(i)}_j,\log\hat{\mathbf{y}}^{(i)}_j)=-\sum_{j=1}^q y^{(i)}_j\log\hat{\mathbf{y}}^{(i)}_j
$$
假设训练数据集的样本数为 $n$，则交叉熵损失函数的定义为：
$$
l(\mathbf{\Theta})=\frac1n\sum_{i=1}^n H(\mathbf{y}^{(i)},\hat{\mathbf{y}}^{(i)})
$$
最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。

### 3.4.6 模型预测及评价

### 小结

1.  Softmax 回归适用于分类问题，使用 Softmax 函数输出类别的概率分布
2.  Softmax 回归是一个单层神经网络，输出个数等于分类问题中的类别个数
3.  交叉熵适合衡量两个概率分布的差异