{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 10.5. Multi-Head Attention\n",
    "注意力机制联合使用查询、键和值的不同表示子空间。使用 $h$ 个独立学习的线性投影来转换查询、键和值，其输出并行地输入注意力池化，将 $h$ 个注意力池化的输出连接起来，使用另一个线性投影进行转换以产生最终输出。\n",
    "### 10.5.1. Model\n",
    "每个注意力的头 $\\mathbf{h}_i$ ($i = 1, \\ldots, h$) 的计算方法为\n",
    "$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$\n",
    "参数 $\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$ 将 $h$ 个头连接在一起：\n",
    "$$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1 \\vdots \\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from mxnet import autograd, np, npx\n",
    "from mxnet.gluon import nn\n",
    "from d2l import mxnet as d2l\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "source": [
    "### 10.5.2. Implementation\n",
    "为多头注意力的每个头选择使用缩放的“点－积”注意力。设置了 $p_q = p_k = p_v = p_o / h$。将查询、键和值的线性变换的输出数量设置为 $p_q h = p_k h = p_v h = p_o$，则可以并行计算 $h$ 头。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Block):\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, use_bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)\n",
    "        self.W_k = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)\n",
    "        self.W_v = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)\n",
    "        self.W_o = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # 'queries' 的形状：('batch_size', 查询或者“键－值”对的个数, 'num_hiddens')\n",
    "        # 'valid_lens' 的形状：('batch_size',) 或者 ('batch_size', 查询的个数)\n",
    "        # 变换后，输出的 'queries', 'keys', 'values' 的形状：\n",
    "        # ('batch_size'*'num_heads', 查询或者“键－值”对的个数, 'num_hiddens'/'num_heads')\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # 在 axis=0，拷贝第一项（标题或者失量）'num_heads' 次；然后拷贝下一项；等等\n",
    "            valid_lens = valid_lens.repeat(self.num_heads, axis=0)\n",
    "\n",
    "        # 'output' 的形状：('batch_size'*'num_heads', 查询的个数, 'num_hiddens'/'num_heads')\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # 'output_concat' 的形状：('batch_size', 查询的个数, 'num_hiddens')\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个转置函数\n",
    "# transpose_output 函数是 transpose_qkv 函数的逆操作\n",
    "def transpose_qkv(X, num_heads):\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    X = X.transpose(0, 2, 1, 3)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"'transpose_qkv' 的逆操作\"\"\"\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.transpose(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 4, 100)\n"
     ]
    }
   ],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
    "attention.initialize()\n",
    "\n",
    "batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, np.array([3, 2])\n",
    "X = np.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = np.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "print(attention(X, Y, Y, valid_lens).shape)"
   ]
  }
 ]
}