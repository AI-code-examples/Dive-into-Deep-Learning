# 10. Attention Mechanisms

## 10.2. Attention Pooling: Nadaraya-Watson Kernel Regression

查询（自主提示）和键（非自主提示）之间的交互产生了「注意力池化」，从而有选择性地聚合了值（感官输入）以产生输出。

### 10.2.3. Nonparametric Attention Pooling

使用 Nadaraya-Watson 核回归，核为 $K(u)=\frac1{\sqrt{2\pi}}\exp(-\frac{u^2}2)$，根据输入的位置对输出进行权衡：
$$
\begin{aligned}
f(x) &= \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i\\
        &=\sum_{i=1}^n\alpha(x,x_i)y_i\\
        &=\sum_{i=1}^n\frac{\exp(-\frac12(x-x_i)^2)}{\sum_{j=1}^n\exp(-\frac12(x-x_j)^2)}y_i\\
        &=\sum_{i=1}^n\mathrm{Softmax}(-\frac12(x-x_i)^2)y_i
\end{aligned}
$$

```python
# X_repeat 的形状: (n_test, n_train), 用于批量计算 $f(x)$，而不是一个个计算
# 每一行都包含着相同的测试输入，即同样的查询
X_repeat = x_test.repeat(n_train).reshape((-1, n_train))
# x_test 包含的是查询；x_train 包含的是键；y_train 包含的是值。
# attention_weights 的形状：(n_test, n_train), 
# 每一行包含的是在给定的每个键 (x_train) 对应的值 (y_train) 之间应该分配的注意力权重
# 当查询与键越接近时，注意力池化的注意力权重就越高
# 用于计算查询 (x_test) 对应的值 (y_hat) 的输出
# 注：由于输入的查询受所有的键和当前位置的值的影响，因此同样的查询输出不同的值
attention_weights = npx.softmax(-(X_repeat - x_train) ** 2 / 2)
# y_hat 的每个元素都是值的加权平均值，其中的权重是注意力权重
y_hat = np.dot(attention_weights, y_train)
```

### 10.2.4. Parametric Attention Pooling

-   非参数 Nadaraya-Watson 核回归具有 *一致性* 的优势：如果有足够的数据，此模型会收敛到最佳解决方案。
-   参数化注意力池化：对非参数模型加入参数 $w$，在大注意力权重位置输出的图形会更加尖锐

$$
\begin{aligned}
f(x) &= \sum_{i=1}^n \alpha(x, x_i)y_i  \\
    &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_i)w)^2\right)} y_i \\
    &= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i.
\end{aligned}
$$

#### 10.2.4.1. Batch Matrix Multiplication

批处理乘法用于提升小批量注意力计算的效率。

假设第一个小批量包含 $n$ 个矩阵 $\mathbf{X}_1,\ldots, \mathbf{X}_n$，形状为 $a\times b$，第二个小批量包含 $n$ 个矩阵 $\mathbf{Y}_1, \ldots, \mathbf{Y}_n$，形状为 $b\times c$。它们的批量矩阵乘法得出 $n$ 个矩阵 $\mathbf{X}_1\mathbf{Y}_1, \ldots, \mathbf{X}_n\mathbf{Y}_n$，形状为 $a\times c$。因此，假定两个张量的形状 $(n,a,b)$ 和 $(n,b,c)$ ，它们的批量矩阵乘法输出的形状为 $(n,a,c)$。

#### 10.2.4.2. Defining the Model

```python
class NWKernelRegression(nn.Block):
    def __init__(self, **kwargs):
        super(NWKernelRegression, self).__init__(**kwargs)
        self.attention_weights = None
        self.w = self.params.get('w', shape=(1,))

    def forward(self, queries, keys, values):
        # 'queries' 的形状: (查询的个数，“键－值”对的个数)
        queries = queries.repeat(keys.shape[1]).reshape((-1, keys.shape[1]))
        # 'attention_weights' 的形状: (查询的个数，“键－值”对的个数)
        self.attention_weights = npx.softmax(-((queries - keys) * self.w.data()) ** 2 / 2)
        # 'values' 的形状: (查询的个数，“键－值”对的个数)
        return npx.batch_dot(np.expand_dims(self.attention_weights, 1), 
                             np.expand_dims(values, -1)).reshape(-1)
```

#### 10.2.4.3. Training

```python
net = NWKernelRegression()
net.initialize()
loss = gluon.loss.L2Loss()
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})
animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])
for epoch in range(50):
    with autograd.record():
        l = loss(net(x_train, keys, values), y_train)
    l.backward()
    trainer.step(1)
```

## 10.3. Attention Scoring Functions

假设查询 $\mathbf{q} \in \mathbb{R}^q$ 和 “键－值”对 $(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m), \mathbf{k}_i \in \mathbb{R}^k, \mathbf{v}_i \in \mathbb{R}^v$。注意力池化函数 $f$ 被实例化为值的加权和：

$$
f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i \in \mathbb{R}^v,
$$

其中查询 $\mathbf{q}$ 和键 $\mathbf{k}_i$ 的注意力权重（标量）是通过注意力评分函数 $a$ 的 softmax 运算得到的，该函数将两个向量映射到标量：

$$
\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}i))}{\sum_{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}.
$$

选择不同的注意力评分函数 $a$ 导致不同的注意力池化行为。

### 10.3.1. Masked Softmax Operation

