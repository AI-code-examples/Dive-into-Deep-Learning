{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('mxnetgpu': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0a85ac76e7f65d073279bd98d1bef912395950828fb0e57c449d1150890cf387"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 8.4 Recurrent Neural Networks\n",
    "### 8.4.1 Neural Networks without Hidden States\n",
    "\n",
    "$$\\mathbf{H}=\\phi(\\mathbf{XW}_{xh}+\\mathbf{b}_h)$$\n",
    "\n",
    "$$\\mathbf{O}=\\mathbf{HW}_{hq}+\\mathbf{b}_q$$\n",
    "\n",
    "### 8.4.2 Recurrent Neural Networks with Hidden States\n",
    "\n",
    "$$\\mathbf{H}_t=\\phi(\\mathbf{X}_t\\mathbf{W}_{xh}+\\mathbf{H}_{t-1}\\mathbf{W}_{hh}+\\mathbf{b}_h$$\n",
    "\n",
    "$$\\mathbf{O}_t=\\mathbf{H}_t\\mathbf{W}_{hq}+\\mathbf{b}_q$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-2.524012  ,  2.5171356 ,  4.6078534 , -0.2956891 ],\n",
       "       [ 0.03744125,  3.84428   ,  1.1564783 ,  2.189958  ],\n",
       "       [ 0.37973815, -2.796899  ,  5.916139  , -2.3432026 ]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "X, W_xh = np.random.normal(0, 1, (3, 1)), np.random.normal(0, 1, (1, 4))\n",
    "H, W_hh = np.random.normal(0, 1, (3, 4)), np.random.normal(0, 1, (4, 4))\n",
    "np.dot(X, W_xh) + np.dot(H, W_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-2.524012  ,  2.5171356 ,  4.607854  , -0.2956891 ],\n",
       "       [ 0.03744116,  3.84428   ,  1.1564783 ,  2.189958  ],\n",
       "       [ 0.3797381 , -2.7968993 ,  5.916139  , -2.3432024 ]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "np.dot(np.concatenate((X, H), 1), np.concatenate((W_xh, W_hh), 0))"
   ]
  },
  {
   "source": [
    "### 8.4.3 RNN-based Character-Level Language Models\n",
    "### 8.4.4 Perplexity\n",
    "\n",
    "交叉熵（cross-entropy）：$$\\frac1n\\sum_{t=1}^n-\\log P(x_t|x_{t-1},\\cdots,x_1)$$\n",
    "\n",
    "困惑度（perplexity）：$\\exp(-\\frac1n\\sum_{t=1}^n\\log P(x_t|x_{t-1},\\cdots,x_1))$\n",
    "\n",
    "困惑度是对交叉熵损失函数做指数运算后得到的值，用于评价语言模型的质量：\n",
    "\n",
    "1.  最佳情况下，模型总是把标签类别的概率预测为 1，此时困惑度也为1；\n",
    "2.  最坏情况下，模型总是把标签类别的概率预测为 0，此时困惑度为 $+\\infty$；\n",
    "3.  基线情况下，模型总是把所有类别的概率预测为相同，此时困惑度为类别个数。\n",
    "\n",
    "注：任何一个有效的模型的困惑度必须小于类别个数。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}